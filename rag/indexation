import os
import json
from tqdm import tqdm
import chromadb
from chromadb.config import Settings

# === PARAM√àTRES ===
CHROMA_PATH = "/home/jerem/Bureau/DecentrIA/DecentrIA_RAG/data/chroma_global"
BASE_PATH = "/home/jerem/Bureau/DecentrIA/DecentrIA_RAG/donnees_brutes/wikipedia_en_clean/articles_par_categorie"
BATCH_SIZE = 5000

# === INITIALISATION CLIENT ===
chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)

# === FONCTION BATCH SAFE ===
def indexer_par_batch(ids, docs, embeddings, collection):
    for i in range(0, len(docs), BATCH_SIZE):
        try:
            collection.add(
                ids=ids[i:i+BATCH_SIZE],
                documents=docs[i:i+BATCH_SIZE],
                embeddings=embeddings[i:i+BATCH_SIZE]
            )
        except Exception as e:
            print(f"‚ùå Erreur batch {i}-{i+BATCH_SIZE}: {e}")

# === REPRISE AUTOMATIQUE : CHARGER LES IDS D√âJ√Ä INDEX√âS ===
def charger_ids_existants(collection):
    ids_existants = set()
    try:
        existants = collection.get(include=[])
        ids_existants.update(existants["ids"])
    except:
        pass
    return ids_existants

# === INDEXATION FICHIERS VECTORIS√âS ===
collections_creees = []

for root, dirs, files in os.walk(BASE_PATH):
    for fichier in files:
        if fichier.endswith("_blocs_vecteurs.jsonl"):
            chemin_fichier = os.path.join(root, fichier)
            nom_collection = os.path.relpath(root, BASE_PATH).replace("/", "_")
            collection = chroma_client.get_or_create_collection(name=nom_collection)
            ids_existants = charger_ids_existants(collection)

            print(f"üìÇ {nom_collection} ‚Üê {fichier}")
            total_lignes = sum(1 for _ in open(chemin_fichier, "r", encoding="utf-8"))
            avec_erreurs = 0
            indexes = 0

            ids_batch = []
            docs_batch = []
            embeds_batch = []

            with open(chemin_fichier, "r", encoding="utf-8") as f:
                for i, ligne in enumerate(tqdm(f, total=total_lignes, unit="vec")):
                    try:
                        data = json.loads(ligne)
                        id_ = data.get("id")
                        texte = data.get("text") or data.get("content")
                        embedding = data.get("embedding")

                        if (
                            isinstance(id_, str)
                            and id_ not in ids_existants
                            and isinstance(texte, str)
                            and isinstance(embedding, list)
                            and all(isinstance(x, (float, int)) for x in embedding)
                        ):
                            ids_batch.append(id_)
                            docs_batch.append(texte)
                            embeds_batch.append(embedding)

                            if len(docs_batch) >= BATCH_SIZE:
                                indexer_par_batch(ids_batch, docs_batch, embeds_batch, collection)
                                ids_existants.update(ids_batch)
                                indexes += len(docs_batch)
                                ids_batch, docs_batch, embeds_batch = [], [], []
                        else:
                            avec_erreurs += 1
                    except Exception as e:
                        avec_erreurs += 1
                        print(f"‚ö†Ô∏è Ligne {i} invalide dans {fichier} ‚Üí {e}")

            # Dernier lot
            if docs_batch:
                indexer_par_batch(ids_batch, docs_batch, embeds_batch, collection)
                indexes += len(docs_batch)

            collections_creees.append((nom_collection, indexes, total_lignes, avec_erreurs))

# === R√âSUM√â FINAL ===
print("\nüìä R√âCAPITULATIF INDEXATION :")
for nom, ajout√©s, total, erreurs in collections_creees:
    print(f"‚úÖ {nom} : {ajout√©s} index√©s sur {total} (‚ö† {erreurs} erreurs)")
