# Module LLM Local

Ce module contient les scripts et configurations nécessaires pour exécuter un modèle de langage (LLM) en local, sans dépendance à une API externe.
Il constitue la base de DecentrIA en garantissant un foctionnement hors-ligne, sécurisé et respectueux de la vie privée.

## Contenu prévu

- Scripts d'initialisation du modèle local
- Gestion de la mémoire et du contexte
- Optimisation GPU (CUDA) et CPU
- Chargement des modèles quantisés (`.gguf`, `.bin`)

### Objectif

Fournir une IA locale robuste, performante et totalement indépendante d'internet.
